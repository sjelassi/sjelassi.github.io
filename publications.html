<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0031)https://colinraffel.com/cv.html -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        
        <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
        <title>Samy Jelassi: Publications</title>
        <link rel="stylesheet" type="text/css" href="./website_files/publications.css">
    </head>
    <body>

        <h1 id="name">Publications</h1> &nbsp; <span class="docname"></span>

        <div class="vline"></div>
 
        <div class="block">

        <div class="item">
                Rachit Bansal, Aston Zhang, Rishabh Tiwari, Lovish Madaan, Sai Surya Duvvuri, Fnu Devvrit, David Brandfonbrener, David Alvarez-Melis, Prajjwal Bhargava, Mihir Kale, <b>Samy Jelassi</b>, <u><a href="https://arxiv.org/abs/2512.13898">
                    “Let's (not) just put things in Context: Test-time Training for Long-context LLMs”</a></u>, <i>submitted</i>, 2025.
        </div>

        <br>        

        <div class="item">
            Parsa Mirtaheri<sup>*</sup>, Ezra Edelman<sup>*</sup>, 
            <b>Samy Jelassi</b>, Eran Malach, Enric Boix-Adsera 
            <u><a href="https://arxiv.org/abs/2505.21825">
                “Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones”
            </a></u>, 
            <i>39th Conference on Neural Information Processing Systems (NeurIPS)</i>, 2025.
            <br>
            <span style="font-size: 0.85em;">
                <sup>*</sup>Equal contribution
            </span>
        </div>

        <br>
        
        
        <div class="item">
            Rosie Zhao<sup>*</sup>, Alexandru Meterez<sup>*</sup>, Sham Kakade, Cengiz Pehlevan, 
            <b>Samy Jelassi</b><sup>†</sup>, and Eran Malach<sup>†</sup>, 
            <u><a href="https://arxiv.org/abs/2504.07912">
                “Echo chamber: RL post-training amplifies behaviors learned in pretraining”
            </a></u>, 
            <i>2nd Conference on Language Modeling (COLM)</i>, 2025.
            <br>
            <span style="font-size: 0.85em;">
                <sup>*</sup>Equal contribution &nbsp;|&nbsp; <sup>†</sup>Equal senior contribution
            </span>
        </div>


            <br> 

          <div class="item">
                 Tian Qin, David Alvarez-Melis<sup>†</sup>, <b>Samy Jelassi</b><sup>†</sup>, and Eran Malach<sup>†</sup>, 
                    <u><a href="https://arxiv.org/abs/2504.07052">
                        “To backtrack or not to backtrack: When sequential search limits model reasoning”
                    </a></u>, 
                    <i>2nd Conference on Language Modeling (COLM)</i>, 2025.
                    <br>
                    <span style="font-size: 0.85em;">
                        <sup>†</sup>Equal senior contribution
                    </span>
           </div>

            <br> 
        

           <div class="item">
                Noah Golowich, <b>Samy Jelassi</b>, David Brandfonbrener, Sham Kakade, Eran Malach, <u><a href="https://arxiv.org/abs/2502.16792">
                    “The Role of Sparsity for Length Generalization in Transformers”</a></u>, <i>42nd International Conference on Machine Learning (ICML)</i>, 2025.
            </div>

            <br>


            <div class="item">
                 Kaiying Hou, David Brandfonbrener, Sham Kakade, <b>Samy Jelassi</b><sup>†</sup>, and Eran Malach<sup>†</sup>, 
                    <u><a href="https://arxiv.org/abs/2407.03310">
                        “Universal Length Generalization with Turing Programs”
                    </a></u>, 
                    <i>42nd International Conference on Machine Learning (ICML)</i>, 2025.
                    <br>
                    <span style="font-size: 0.85em;">
                        <sup>†</sup>Equal senior contribution
                    </span>
           </div>

            <br>
            
            <div class="item">
                <b>Samy Jelassi</b>, Clara Mohri, David Brandfonbrener, Alex Gu, Nikhil Vyas, Nikhil Anand, David Alvarez-Melis, Yuanzhi Li, Sham M. Kakade, and Eran Malach, <u><a href="https://arxiv.org/abs/2410.19034">
                    “Mixture of Parrots: Experts improve memorization more than reasoning”</a></u>, <i>13th International Conference on Learning Representations (ICLR), 2025.
                <b>Oral presentation (top 10%)</b> at  at the “Mathematics of modern machine learning” workshop, NeurIPS 2024.</i> 
            </div>

            <br>
            
            <div class="item">
                Jyothish Pari, <b>Samy Jelassi</b>, and Pulkit Agrawal, <u><a href="https://arxiv.org/abs/2411.02207">
                    “Collective Model Intelligence Requires Compatible Specialization”</a></u>, <i>In submission</i>, 2024.
            </div>

            <br>
            
            <div class="item">
                Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham Kakade, Eran Malach, and <b>Samy Jelassi</b>, <u><a href="https://arxiv.org/abs/2410.13025">
                    “LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks”</a></u>, <i>31st International Conference on Computational Linguistics (COLING), Industry track</i>, 2025.
            </div>

            <br>
            
            
            <div class="item">
                Kenneth Li, <b>Samy Jelassi</b>, Hugh Zhang, Sham Kakade, Martin Wattenberg, and David Brandfonbrener, <u><a href="https://arxiv.org/abs/2402.14688">
                    “Q-Probe: A Lightweight Approach to Reward Maximization for Language Models”</a></u>, <i>41st International Conference on Machine Learning (ICML),</i> 2024.
            </div>

            <br>

            <div class="item">
                <b>Samy Jelassi</b>, David Brandfonbrener, Sham M. Kakade, and Eran Malach, <u><a href="https://arxiv.org/abs/2402.01032">
                    “Repeat After Me: Transformers are Better than State Space Models at Copying”</a></u>, <i>41st International Conference on Machine Learning (ICML),</i> 2024.
            </div>

            <br>

            <div class="item">
                <b>Samy Jelassi</b>, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François Charton, <u><a href="https://arxiv.org/abs/2306.15400">
                    “Length Generalization in Arithmetic Transformers”</a></u>, 2023.
            </div>

            <br>

            <div class="item">
                <b>Samy Jelassi</b>, Boris Hanin, Ziwei Ji, Sashank J. Reddi, Srinadh Bhojanapalli, and Sanjiv Kumar, <u><a href="https://arxiv.org/abs/2305.07810">
                    “Depth Dependence of μP Learning Rates in ReLU MLPs”</a></u>, 2023.
            </div>

            <br>

            <div class="item">
                <b>Samy Jelassi</b>, Michael Sander and Yuanzhi Li, <u><a href="https://arxiv.org/abs/2210.09221">
                    “Vision Transformers provably learn spatial structure”</a></u>, <i>36th Conference on Neural Information Processing Systems (NeurIPS),</i> 2022.
            </div>

            <br>

            <div class="item">
                <b>Samy Jelassi*</b>, David Dobre*, Arthur Mensch, Yuanzhi Li, and Gauthier Gidel, <u><a href="https://arxiv.org/abs/2210.04319">
                    “Dissecting adaptive methods in GANs”</a></u>, 2022.
            </div>

            <br>

            <div class="item">
                <b>Samy Jelassi</b>, and Yuanzhi Li, <u><a href="https://arxiv.org/abs/2210.09221">
                    “Towards understanding how momentum improves generalization in deep learning”</a></u>, <i>39th International Conference on Machine Learning (ICML),</i> 2022.<br>
                 <i><b>Oral presentation (top 5%)</b> at  at the “Overparameterization: Pitfalls & Opportunities” workshop, ICML 2021.</i>    
            </div>

            <br>

            <div class="item">
                Luca Venturi, <b>Samy Jelassi</b>, Tristan Ozuch, and Joan Bruna, <u><a href="https://arxiv.org/abs/2210.09221">
                    “Depth separation beyond radial functions”</a></u>, <i>Journal of Machine Learning Research (JMLR),</i> 2022.
            </div>

            <br>

            <div class="item">
                Aaron Defazio, and Samy Jelassi, <u><a href="https://arxiv.org/abs/2101.11075">
                    “Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization”</a></u>, <i>Journal of Machine Learning Research (JMLR),</i> 2022.
            </div>

            <br>

            <div class="item">
                Jad Rahme, <b>Samy Jelassi</b>, and S. Matthew Weinberg, <u><a href="https://arxiv.org/abs/2006.05684">
                    “Auction learning as a two-player game”</a></u>, <i>9th International Conference on Learning Representations (ICLR),</i> 2021.
            </div>

            <br>

            <div class="item">
                Jad Rahme, <b>Samy Jelassi</b>, Joan Bruna, and S. Matthew Weinberg, <u><a href="https://arxiv.org/abs/2003.01497">
                    “A Permutation-Equivariant Neural Network Architecture For Auction Design”</a></u>, <i>35th AAAI Conference on Artificial Intelligence,</i> 2021.
            </div>

            <br>

            <div class="item">
                <b>Samy Jelassi</b>, Carles Domingo-Enrich, Damien Scieur, Arthur Mensch, Joan Bruna, <u><a href="https://arxiv.org/abs/1905.12363">
                    “Extragradient with player sampling for faster Nash equilibrium finding”</a></u>, <i>37th International Conference on Machine Learning (ICML)</i> 2020.
            </div>

            <br>

            <div class="item">
                Carles Domingo-Enrich,  <b>Samy Jelassi</b>, Arthur Mensch, Grant Rotskoff, and Joan Bruna, <u><a href="https://arxiv.org/abs/2002.06277">
                    “A mean-field analysis of two-player zero-sum games”</a></u>, <i>33rd Conference on Neural Information Processing Systems (NeurIPS)</i> 2019.
            </div>

            <br>

            <div class="item">
                Othmane Sebbouh, Nidham Gazagnadou, <b>Samy Jelassi</b>, Francis Bach, and Robert M. Gower, <u><a href="https://arxiv.org/abs/1908.02725">
                    “Towards closing the gap between the theory and practice of SVRG”</a></u>, <i>33rd Conference on Neural Information Processing Systems (NeurIPS)</i> 2019.
            </div>

            <br>

            <div class="item">
                Grant Rotskoff, <b>Samy Jelassi</b>, Joan Bruna, and Eric Vanden-Eijnden, <u><a href="https://arxiv.org/abs/1902.01843">
                    “Global convergence of neuron birth-death dynamics”</a></u>, <i>36th International Conference on Machine Learning (ICML)</i> 2019.
            </div>

            <br>

            <div class="item">
                Thomas Pumir, <b>Samy Jelassi</b>, and Nicolas Boumal, <u><a href="https://arxiv.org/abs/1806.03763">
                    “Smoothed analysis of the low-rank approach for smooth semidefinite programs”</a></u>, <i> <b>Oral presentation (top 3%)</b> at the 32nd Conference on Neural Information Processing Systems (NeurIPS)</i> 2018.
            </div>


            
    
</body></html>
